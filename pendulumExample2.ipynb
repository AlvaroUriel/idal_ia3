{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvaroUriel/idal_ia3/blob/main/pendulumExample2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUciZWFjW84k"
      },
      "outputs": [],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnDE7L5nXr52"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiMBW_WiXuPX",
        "outputId": "015da926-3a94-48e2-faf6-673cf2b96e5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f3863b3eb50>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH4J6lATX1kj"
      },
      "outputs": [],
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNg1isZcUsHt"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "from keras import Sequential\n",
        "from collections import deque\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "env = wrap_env(gym.make('Pendulum-v0'))\n",
        "env.seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXQYshODYgjw"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "\n",
        "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
        "\n",
        "    def __init__(self, action_space, state_space):\n",
        "\n",
        "        self.action_space      = action_space\n",
        "        self.state_space       = state_space\n",
        "        \n",
        "        self.gamma             = 0.95    # Discount factor for estimaing the futures rewards\n",
        "        self.batch_size        = 64      # Size of the batch when sampling experiences\n",
        "\n",
        "        self.epsilon           = 1       # Initial probability for choosing exploration instead of explotation \n",
        "        self.epsilon_min       = 0.05    # Last probability for choosing exploration instead of explotation \n",
        "        self.approx_iterations = 200     # This is an estimation of the training iterations to tune the epsilon decay\n",
        "\n",
        "        # The epsilon_decay reduce the exploration probability after each iteration\n",
        "        self.epsilon_decay = (self.epsilon_min / self.epsilon) ** (1 / self.approx_iterations)\n",
        "\n",
        "        #self.learning_rate = 0.001       # The usual factor that controls the amount of change the weights are updated\n",
        "        self.learning_rate = 0.005       # The usual factor that controls the amount of change the weights are updated\n",
        "\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape=(self.state_space,), activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_space, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = np.array([i[0] for i in minibatch])\n",
        "        actions = np.array([i[1] for i in minibatch])\n",
        "        rewards = np.array([i[2] for i in minibatch])\n",
        "        next_states = np.array([i[3] for i in minibatch])\n",
        "        dones = np.array([i[4] for i in minibatch])\n",
        "\n",
        "        states = np.squeeze(states)\n",
        "        next_states = np.squeeze(next_states)\n",
        "\n",
        "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
        "        targets_full = self.model.predict_on_batch(states)\n",
        "\n",
        "        ind = np.array([i for i in range(self.batch_size)])\n",
        "        targets_full[[ind], [actions]] = targets\n",
        "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFxnJblXYS43"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_dqn(episode):\n",
        "    global env\n",
        "    loss = []\n",
        "    #agent = DQN(env.action_space.n, env.observation_space.shape[0])\n",
        "    agent = DQN(5, env.observation_space.shape[0])\n",
        "    for e in range(episode):\n",
        "        temp=[]\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, (1, 3))\n",
        "        score = 0\n",
        "        maxp = -1.2\n",
        "        max_steps = 1000\n",
        "        for i in range(max_steps):\n",
        "            env.render()\n",
        "            action = agent.act(state)\n",
        "            torque = [-2+action]\n",
        "            next_state, reward, done, _ = env.step(torque)\n",
        "            next_state = np.reshape(next_state, (1, 3))\n",
        "            if (next_state[0,0]>0.95):\n",
        "                score=score+1\n",
        "            reward= 25*np.exp(-1*(next_state[0,0]-1)*(next_state[0,0]-1)/0.001)-100*np.abs(10*0.5 - (10*0.5*next_state[0,0] + 0.5*0.3333*next_state[0,2] * next_state[0,2])) + 100*np.abs(10*0.5 - (10*0.5*state[0,0] + 0.5*0.3333*state[0,2] * state[0,2]))\n",
        "            maxp = max(maxp, next_state[0,0])\n",
        "            temp.append(next_state[0,0])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            agent.replay()\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
        "                print(maxp)\n",
        "                plt.plot([i for i in range(0, 200, 1)], temp[::1])\n",
        "                plt.show()\n",
        "                env.close()\n",
        "                show_video()\n",
        "                env = wrap_env(gym.make('Pendulum-v0'))\n",
        "                env.seed(episode)\n",
        "                break\n",
        "        loss.append(score)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnns4HOcYXZL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def random_policy(episode, step):\n",
        "\n",
        "    for i_episode in range(episode):\n",
        "        env.reset()\n",
        "        for t in range(step):\n",
        "            action = env.action_space.sample()\n",
        "            state, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "                break\n",
        "            print(\"Starting next episode\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVLxbpFHYZKu"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    ep = 150\n",
        "    loss = train_dqn(ep)\n",
        "    plt.plot([i+1 for i in range(0, ep, 2)], loss[::2])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFt-QpoCCpLl"
      },
      "outputs": [],
      "source": [
        "    plt.plot([i+1 for i in range(0, ep, 2)], loss[::2])\n",
        "    plt.xlabel('Episode no.')\n",
        "    plt.ylabel('Time spent in inverted position')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pendulumExample2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}