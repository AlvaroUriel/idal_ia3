{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvaroUriel/idal_ia3/blob/main/pendulumExample3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MIA3: Trabajo Módulo Reinforcement Learning\n",
        "#Trabajo: GymLibray/Classic Control/Pendulum\n",
        "###Alumno: Álvaro Uriel Cárcel\n"
      ],
      "metadata": {
        "id": "mRnvgYsD43MJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introducción\n",
        "En el presente trabajo se tratará de resolver un Clásico Entorno de Teoria de Contról de la libreria Gym. Para ello se emplearán algoritmos de Machine Learning, concretamente algoritmos de Aprendizaje Reforzado o Reinforcement learning.\n",
        "\n",
        "El Entorno que se ha elegido para el desarrollo del trabajo es 'Pendulum'. Este entorno tiene cierta similitud con los que se han trabajado en las clases como son 'Cart Pole' y 'Mountain Car', digamos a priori se podría decir que son de dificultad similar a la hora de su resolución, es por ello que se ha decido emplear este Entorno para el Trabajo. Como su nombre indica el entorno consiste en un Péndulo Invertido que inicia en una posición aleatoria y está sujeto a las leyes de la gravedad, el desafio se evalua contabilizando el tiempo que se ha conseguido mantener el pendulo con su centro de masas por encima del punto fijo respecto a al tiempo del episodio.\n",
        "\n",
        "\n",
        "https://www.gymlibrary.ml/environments/classic_control/pendulum/\n",
        "\n",
        "###Espacio de Acciones:\n",
        "Se puede actuar sobre el Torque del Péndulo en el intervalo [-2,2].\n",
        "\n",
        "\n",
        "###Espacio de Observación:\n",
        "Se puede observar 3 variables: las coordenadas x-y y la velocidad angular de su extremo libre.\n",
        "\n",
        "x = cos(theta) [-1.0, 1.0]\n",
        "\n",
        "y = sin(angle) [-1.0, 1.0]\n",
        "\n",
        "Angular Velocity [-8.0, 8.0]\n"
      ],
      "metadata": {
        "id": "tzZRV0NVvXb-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nUciZWFjW84k"
      },
      "outputs": [],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WnDE7L5nXr52"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from keras import Sequential\n",
        "from collections import deque\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vH4J6lATX1kj"
      },
      "outputs": [],
      "source": [
        "def muestra_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 300px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"No se encontro el video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make('Pendulum-v0'))\n",
        "env.seed(0)\n",
        "np.random.seed(0)\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTH6GqUq872x",
        "outputId": "4f143ab0-4878-4c9a-d3b3-45944137cbbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f0dcdaa13d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UXQYshODYgjw"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "\n",
        "    def __init__(self, action_space, state_space):\n",
        "\n",
        "        self.action_space      = action_space\n",
        "        self.state_space       = state_space\n",
        "        \n",
        "        self.gamma             = 0.95    # Discount factor for estimaing the futures rewards\n",
        "        self.batch_size        = 64      # Size of the batch when sampling experiences\n",
        "\n",
        "        self.epsilon           = 1       # Initial probability for choosing exploration instead of explotation \n",
        "        self.epsilon_min       = 0.05    # Last probability for choosing exploration instead of explotation \n",
        "        self.approx_iterations = 200     # This is an estimation of the training iterations to tune the epsilon decay\n",
        "\n",
        "        # The epsilon_decay reduce the exploration probability after each iteration\n",
        "        self.epsilon_decay = (self.epsilon_min / self.epsilon) ** (1 / self.approx_iterations)\n",
        "\n",
        "        #self.learning_rate = 0.001       # The usual factor that controls the amount of change the weights are updated\n",
        "        self.learning_rate = 0.005       # The usual factor that controls the amount of change the weights are updated\n",
        "\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape=(self.state_space,), activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_space, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = np.array([i[0] for i in minibatch])\n",
        "        actions = np.array([i[1] for i in minibatch])\n",
        "        rewards = np.array([i[2] for i in minibatch])\n",
        "        next_states = np.array([i[3] for i in minibatch])\n",
        "        dones = np.array([i[4] for i in minibatch])\n",
        "\n",
        "        states = np.squeeze(states)\n",
        "        next_states = np.squeeze(next_states)\n",
        "\n",
        "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
        "        targets_full = self.model.predict_on_batch(states)\n",
        "\n",
        "        ind = np.array([i for i in range(self.batch_size)])\n",
        "        targets_full[[ind], [actions]] = targets\n",
        "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PFxnJblXYS43"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_dqn(episode):\n",
        "    global env\n",
        "    loss = []\n",
        "    #agent = DQN(env.action_space.n, env.observation_space.shape[0])\n",
        "    agent = DQN(5, env.observation_space.shape[0])\n",
        "    for e in range(episode):\n",
        "        temp=[]\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, (1, 3))\n",
        "        score = 0\n",
        "        maxp = -1.2\n",
        "        max_steps = 1000\n",
        "        for i in range(max_steps):\n",
        "            env.render()\n",
        "            action = agent.act(state)\n",
        "            torque = [-2+action]\n",
        "            next_state, reward, done, _ = env.step(torque)\n",
        "            next_state = np.reshape(next_state, (1, 3))\n",
        "            if (next_state[0,0]>0.95):\n",
        "                score=score+1\n",
        "                \n",
        "            maxp = max(maxp, next_state[0,0])\n",
        "            temp.append(next_state[0,0])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            agent.replay()\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
        "                print(maxp)\n",
        "                plt.plot([i for i in range(0, 200, 1)], temp[::1])\n",
        "                plt.show()\n",
        "                env.close()\n",
        "                muestra_video()\n",
        "                env = wrap_env(gym.make('Pendulum-v0'))\n",
        "                env.seed(episode)\n",
        "                break\n",
        "        loss.append(score)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xnns4HOcYXZL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def random_policy(episode, step):\n",
        "\n",
        "    for i_episode in range(episode):\n",
        "        env.reset()\n",
        "        for t in range(step):\n",
        "            action = env.action_space.sample()\n",
        "            state, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "                break\n",
        "            print(\"Starting next episode\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVLxbpFHYZKu"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    ep = 20\n",
        "    loss = train_dqn(ep)\n",
        "    plt.plot([i+1 for i in range(0, ep, 2)], loss[::2])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFt-QpoCCpLl"
      },
      "outputs": [],
      "source": [
        "    plt.plot([i+1 for i in range(0, ep, 2)], loss[::2])\n",
        "    plt.xlabel('Episode no.')\n",
        "    plt.ylabel('Time spent in inverted position')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pendulumExample3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}